## 트리

트리는 그래프의 일종으로 노드와 가지로 이루어 져있습니다. 

트리중 가자윗부분 처음에 있는 노드를 "루트노드"라 하고 트리의 가장 아랫부분의 노드를 "리프노드", 그 둘이 아닌 노드를 "내부노드(안쪽 노드)"라고 합니다. 

또한  가지를 기준으로 위의 노드를 "부모노드" 아래의 노드를 "자식 노드"라 합니다.

루트에서 같은 수의 가지를 지난 노드들을 같은 "레벨" 에 있다고 합니다.

트리 구조를 하고 있으면서 자식노드가 2개인 트리를 이진트리라고 합니다.

<right><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Binary_tree.svg/1024px-Binary_tree.svg.png width="50%" height="50%" ></right>
이진트리 탐색의 종류

1. 중위 순회 : 왼쪽 자식 노드에서 부모 노드 오른쪽 자식 노드 순으로 탐색
2. 전위 순회 : 부모 노드에서 왼쪽 자식 노드 오른쪽 자식 노드 순으로 탐색
3. 훈위 순회 : 왼쪽 자식 노드에서 오른쪽 자식 노드 부모 노드 순으로 탐색
4. 레벨 순회 : 부모 노드에서 같은 레벨에 있는 노드들 먼저 검색

# 3.6 결정트리학습

결정트리 란 훈련 데이터에 있는 특성을 기반으로 샘플의 클래스 레이블을 추정할 수 있는 일련의 질문을 학습합니다.

트리의 루트에서 시작해서 정보 이득이 최대가 되는 특성으로 데이터를 나누게 됩니다.

일반적으로, 기대되는 정보 이득은 이전 상태에서 주어진 정보를 갖는 상태로 바뀔 때 정보 엔트로피의 변화 입니다.

반복 과정을 통하여 리프노드가 순수해질 때까지 모든 자식 노드에 이 분할 작업을 반복합니다.

실제로 이렇게 하면 노드가 많은 깊은 트리가 만들어지고 과대적합될 가능성이 높습니다. 일반적으로 트리의 최대 깊이를 제한하여 트리를 가지치기합니다



## 3.6.1 정보 이득 최대화 : 자원을 최대로 활용



### 정보이득(Information Gain)

정보이득이란 어떤 속성을 선택함으로 인해 데이터를 더 잘 구분하게 되는 것을 말합니다. 

정보이득은 다음과 같이 정의합니다.


$$
IG(D_P,f) = I(D_P)-\sum_{j=1}^m{\frac{N_j}{N_P}I(D_j)}
$$
여기서 $f$는 분할에 사용할 특성입입니다. $D_P$와 $D_j$는 부모와 $j$번째 자식 노드의 데이터 셋입니다. $I$는 불순도 지표입니다.

$N_P$는 부모노드의 전체 샘플 개수 $N_j$는 $j$번째 자식 노드에 있는 샘플 개수입니다.

여기서 볼 수 있듯이 정보 이득은 단순히 부모 노드의 불순도와 자식 노드의 불순도 합의 차이입니다.

이진 결정 트리에서는$m$이 2로 $D_{left}$ 와$D_{right}$로 나눠집니다.
$$
IG(D_P,f)=I(D_P)-\frac{N_{left}}{N_P}I(D_{left})-\frac{N_{right}}{N_P}I(D_{right})
$$
이진 결정 트리에 널리 사용되는 세개의 불순도 지표 또는 불할 조건은 지니불순도, 엔트로피, 분류 오차 가있습니다.

### 불순도 지표

##### 엔트로피


$$
I_H(t)=-\sum_{i=1}^c{p(i\mid t)\log_{2}{p(i\mid t)}}
$$
주어진 데이터 집합의 혼잡도를 의미 합니다. 이는 주어진 데이터 집합에서 서로 다른 종류의 레코드들이 섞여 있으면 엔트로피가 높고 , 같은 종류의 레코드들이 섞여 있으면 엔트로피가 낮아집니다.

엔트로피는 0~1 사이 값을 가지며 혼합도가 높아지면 1에, 반대로 낮아지면 0에 가까워 집니다.

최대값인 1이 되려면 $p(i \mid t) = \frac{1}{2}$일때
$$
I_H(t)=-{\frac{1}{2}\log_{2}\frac{1}{2}}-{\frac{1}{2}\log_{2}\frac{1}{2}}=1
$$

$p(i \mid t) = 1$이때
$$
I_H(t)=-{\frac{1}{1}\log_{2}\frac{1}{1}}=0
$$


<right><img src=https://k.kakaocdn.net/dn/bIOfKV/btqv2KmwapD/b11nkL7kkJk4nrRAYqYQ90/img.png width="50%" height="50%"></right>
다른 예시로 위의 그림과 같이 빨간공(10개)과 파란공(6개)을 분류 하게 되면

1. 먼저 점선이 그려지기 전($D_P$)의 엔트로피를 구한다
   $$
   I_H(D_{P})=-{\frac{10}{16}\log_{2}\frac{10}{16}}-{\frac{6}{16}\log_{2}\frac{6}{16}}=0.954
   $$
   
2. 위쪽($D_{left}$)의 엔트로피와 아래쪽($D_{right}$)의 엔트로피를 구하면

$$
I_H(D_{left})=-{\frac{7}{8}\log_{2}\frac{7}{8}}-{\frac{1}{8}\log_{2}\frac{1}{8}}=0.543
$$

$$
I_H(D_{right})=-{\frac{3}{8}\log_{2}\frac{3}{8}}-{\frac{5}{8}\log_{2}\frac{5}{8}}=0.954
$$

3. 정보이득을 구한다.

$$
IG(D_P,f)=0.954-\frac{8}{16}\times 0.543-\frac{8}{16}\times 0.954=0.205
$$



##### 지니 불순도

지니 불순도는 집합에 이질적인 것이 얼마나 섞였는지를 측정하는 지표로 어떤 집합에서 한 항목을 뽑아 무작위로 라벨을 추정할 때 츨릴 확률을 말합니다. 집합에 있는 항목이 모두 같다면 지니 불순도는 최솟값(0)을 갖게 되며 이집합은 완전히 순수 하다고 할 수 있습니다.

$$
I_G(t)=\sum_{i=1}^cp(i\mid t)(1-p(i \mid t))= 1-\sum_{i=1}^cp(i \mid t)^2
$$


엔트로피와 비슷하게 지니 불순도는 클래스가 균일하게 섞여 있을 때 최대 가 됩니다. 

최대값이 되려면 $p(i \mid t) = \frac{1}{2}$일때
$$
I_G(t)=1-\bigg(\frac{1}{2}\bigg)^2-\bigg(\frac{1}{2}\bigg)^2=0.5
$$

또한 최소값은 $p(i \mid t) = 1$일때
$$
I_H(t)=1-\bigg(\frac{1}{1}\bigg)^2=0
$$
엔트로피에 비해 1/2인 0.5가 최대값입니다.

위의 그림을 예로 들면

1. 먼저  점선이 그려지기 전($D_P$)의 지니 불순도를 구한다
   $$
   I_G(G_P)=\sum_{i=1}^cp(i\mid t)(1-p(i \mid t))= 1-\sum_{i=1}^cp(i \mid t)^2=1-\bigg(\frac{10}{16}\bigg)^2- \bigg(\frac{6}{16}\bigg)^2=0.469
   $$

2. 위쪽($D_{left}$)의 엔트로피와 아래쪽($D_{right}$)의 엔트로피를 구하면
   $$
   I_G(D_{left})=1-\sum_{i=1}^cp(i \mid t)^2=1-\bigg(\frac{7}{8}\bigg)^2- \bigg(\frac{1}{8}\bigg)^2=0.219
   $$

   $$
   I_G(D_{right})=1-\sum_{i=1}^cp(i \mid t)^2=1-\bigg(\frac{3}{8}\bigg)^2- \bigg(\frac{5}{8}\bigg)^2=0.469
   $$

   

3. 정보이득을 구한다.

$$
IG(D_P,F)=0.469-\frac{8}{16}\times0.219-\frac{8}{16}\times0.469=0.125
$$

##### 분류오차

$$
I_E=1-\max{\{p(i\mid t)\}}
$$

가지치기에는 좋은 기준이지만 결정 트리를 구성하는데는 권장되지 않습니다. 또한 나머지 두 지표와 달리 미분이 불가능하다는 점에 자주 쓰이지 않는다고 합니다.

분류오차의 최대값 또한

$p(i \mid t) = \frac{1}{2}$일 때 최대값을 가집니다.
$$
I_E=1-\max\bigg\{\frac{1}{2}\bigg\}=0.5
$$
$p(i \mid t) = 1$일때
$$
I_E=1-\max\bigg\{\frac{1}{1}\bigg\}=0
$$
위의 그림을 예로 들면
$$
I_E(D_P)=1-\max\bigg\{\frac{10}{16}\bigg\}=\frac{6}{16}
$$

$$
I_E(D_{left})=1-\max\bigg\{\frac{7}{8}\bigg\}=\frac{1}{8}
$$

$$
I_E(D_{right})=1-\max\bigg\{\frac{5}{8}\bigg\}=\frac{3}{8}
$$

$$
IG(D_P,F)=\frac{6}{16}-\frac{8}{16}\frac{1}{8}-\frac{8}{16}\frac{3}{8}=0.125
$$

<right><img src="Screenshot from 2019-12-02 02-39-38.png"></right>



<right><img src="Screenshot from 2019-12-02 02-41-23.png"></right>