# 6장 복합 출력의 처리 방법

 ## 다차원 분류

먼저 이 장에서 사용될 데이터인 오피스 31인 전이학습 연구용으로 구축된 표준 벤치마크 데이터셋으로 사무용품 이미지로 구성되있다고 합니다. 

전이학습이란  학습데이터가 부족한 분야의 모델 구츨을 위해 데이터가 풍부한 분야에서 훈련된 모델을 재사용하는 기법입니다.

오피스31에서는 한도메인에서 학습시킨 결과를 다른 도메인에 활용하여 학습효과를 높이는데 활용됩니다.

하지만 이 장에서는 전이학습보단 각 데이터 이미지의 도메인과 품목을 동시에 판별해주는 즉 두가지 차원에서의 분류를 한번에 수행을 하고 결과를 동시에 보여주게 됩니다.

## 복합출력의 학습법

동일한 구조의 신경망을 다양한 용도에서 후처리 과정에서의 처리방법만 변경하면 됩니다. 즉 순전파에서 손실함수 구하는 과정 역전파에서 출력의 각 성분의 손실기울기를 구하는 과정의 변경입니다.

출력별로 별도의 신경망을 구성하게 되도 임무에 따라 역할이 확실하게 다르게 때문에 출력 계층만 따지면 신경망을 분리하여도 상관없을 것입니다. 하지만 출력은 여럿이지만 이들 출력이 내용상 서로 연관되어 있다 보니 중요한 공통 특성이 있을 수 있습니다. 따라서  각각의 출력을 위한 별도의 은닉 계층이 이러한 공통 특성을 포착하도록 학습시키면 계산량을 절감하면서도 성능을 향상시킬 수 있습니다. 즉 출력별로 별도의 신경망을 구성한는 것보다는 하나의 신경망 출력을 복합 출력으로 처리하는 것이 좋습니다.

복합출력을 낼때 학습 방법은 레이블 정보로 준비된 정답 $y_1, y_2$와 복합 출력의 두 성분 $output_1,output_2$를 비교해 손실값 $L_1,L_2$가 얻어진다고 할 때 전체적인 손실 함숫값은
$$
L=L_1+L_2
$$
으로 정의하면 된다.



## Optimizer

optimizer는 경사하강법으로 가중치를 업데이트하는 알고리즘들 입니다.  우리는 지금까지 매개 변수의 기울기를 이용해 기우러진 방향으로 매개변수 값을 갱신하는 일을 반복해 최적화를 진행했다. 이것이 확률적 경사 하강법(SGD)입니다. 그 외에 모멘텀(Momentum), AdaGrad(Adaptive Gradient),RMSprop 이 있습니다.



### SGD(확률적 경사 하강법)

SGD는 한번 학습할 때 모든 데이터에 대한 가중치를 조절하는 것이 아니라 램덤하게 추출한 일부 데이터에 대해 가중치를 조절합니다. 결과적으로 속도는 개선 되지만 최적 해의 정확도는 낮아 집니다.
$$
W \leftarrow W - \eta{\partial L \over \partial W}
$$

> 파이썬 소스 코드
>
> ```python
> weight -= LEARNING_RATE * G_w	
> ```



장점으로는 기울기 수정시 훈련 데이터 중에서 무작위로 샘플을 선택하기 때문에 국소 최적해에 잘 빠지지 않습니다. 단점으로는 다른 알고리즘처럼 학습률을 자동으로 조정해주는 파라미터가 없이 수정량의 유연한 조정이 불가능하다고 합니다.

![SGD](.\SGD.png)



### Momentum

모멘텀은 경사 하강법에서 관성을 더해주는 것입니다. 경사 하강법과 마찬가지로 매번 기울기를 구하지만, 가중치를 수정하기 전 이전 방향을 참고하여 같은 방향으로 일정한 비율만 수정하게 하는 방법입니다. 수정이 양방향 음방향 순차적으로 일어나는 지그재그 현상이 줄고 이전 이동값을 고려해서 일정 비율만큼 다음 값을 결정하므로 관성의 효과를 낼 수 있습니다.
$$
v \leftarrow \alpha v - \eta {\partial L \over \partial W}\\
W \leftarrow W +v
$$
여기서 $\alpha$는 모멘텀 계수로 보통0.9로 설정합니다.

> 파이썬 소스 코드
>
> ```python
> v = m * v - LEARNING_RATE * G_w	
> weight -= v
> ```



장점으로는 관성항을 더함으로써 새로운 수정량은 이전까지의 수정량으로부터 영향을 받게 되고

수정량이 급격하게 변화하는것을 막아줍니다.

단점으로는 수정해야할 상수가 늘어나 조정이 조금 어렵습니다.

![Moentum](.\Moentum.png)

### AdaGrad(Adaptive Gradient)

아다그라드는 가중치의 업데이트 횟수에 따라 학습률를 조절하는 옵션이 추가된 최적화 방법입니다. 아다그라드는 많이 변화하지 않는 변수들은 학습률을 크게하고 반대로 많이 변화한 변수들에 대해서는 학습률을 적게 합니다. 이는 많이 변화한 변수는 최적값에 근접했을 것이라는 가정하에 작은 크기로 이동하면서 세밀한 값을 조정하고, 반대로 적게 변화한 변수들은 학습률을 크게하여 빠르게 loss값을 줄입니다.
$$
g \leftarrow g +\left(\partial L \over \partial W\right)^2\\
W \leftarrow W - \eta{1\over\sqrt {g + \epsilon}}{\partial L \over \partial W}
$$


> 파이썬 소스 코드
>
> ```python
> g += G_w * 2
> weight += - LEARNING_RATE( G_w / (np.sart(g) + e))
> ```



장점으로는 효율성이 좋고 상수가 학습률만 있으므로 상수 조정 번거로움이 적습니다.

단점은 계속되는 g값의 증가로 도중에 weight가 0이 되어버려 최적화가 더이상 진행되지 않는 단점이 존재합니다.

![AdaGrad](.\AdaGrad.png)



### RMSProp

이는 아다그라드의 $g$의 값이 무한히 커지는 것을 방지하고자 제안된 방법입니다.
$$
g = \alpha g +(1-\alpha)\left({\partial L \over \partial W}\right)^2\\

W \leftarrow W - \eta{1\over\sqrt {g + \epsilon}}{\partial L \over \partial W}
$$


> 파이썬 소스 코드
>
> ```python
> g += a * g + (1 - a)* np.square(G_w)
> weight += - LEARNING_RATE*G_w / (np.sqrt(g) + e)
> ```

### Adam

앞에서 언급했던 momentum과 RMSProp을 섞은 기법입니다. 따라서 하이퍼파라미터도 그만큼 많습니다. 모멘텀에서서 사용하는 계수와 학습률에 대한 계수가 사용됩니다. 학습률을 줄여나가고 속도를 계산하여 학습의 갱신강도를 적응적으로 조정해나가는 방법입니다.

$$
m \leftarrow \beta_1 m + (1 -\beta_1){\partial L \over \partial W}\\
v \leftarrow \beta_2 v + (1 - \beta_2)\left({\partial L \over \partial W}\right)^2\\
\hat{m} \leftarrow m/(1- \beta_1) \ \ \ \  \ 	\hat{v} \leftarrow v/(1-\beta_2)\\
W = W - \eta{\hat{m} \over \sqrt {\hat v +e}}
$$
자세한 내용은 아래 링크에 논문이 있습니다.

https://arxiv.org/pdf/1412.6980.pdf



![Adam](.\Adam.png)