# 1장 회귀 분석

## 1.1  단층 퍼셉트론 신경망 구조

퍼셉트론은 동물의 신경세포인 뉴런을 흉내 내어 고안한 것입니다.

이들 퍼셉트론은 저마다 가중치 벡터와 편향값을 이용하여 입력 벡터**x**로 부터 출력 벡터 **y**  를 도출한다. 

예시로 하나의 퍼셉트론은 벡터 $$\mathbf{x}=(x_1,x_2,x_3,x_4)$$ 이 있으면 가중치 벡터 $$\mathbf{w}=(w_1,w_2,w_3,w_4)$$  과 편향값 b_1을 이용하여 아래 식을 도출 할 수 있습니다.


$$
y_1 = x_1w_1+x_2w_2+x_3w_3+x_4w_4 +b_1
$$
단층 퍼셉트론은 이러한 퍼셉트론이 한줄로 배치하여 입력베터 하나로부터 출력 벡터 하나를 얻어내는 가장 기본적인 신경망 구조입니다.

이때는 가중치백터들을 한데 모아 가중치 행렬**W**를 사용하고 평향값들을 한데 모은 편행 벡터**b**를 사용합니다.

여기서 가중치 행렬과 편향 벡터를 파라미터라고 한다. 이는 학습과정 중에 끊임없이 변경되어 가면서 퍼셉트론의 동작 특성을 결정하는 값들입니다.

단층 퍼셉트론 신경망에서는 퍼셉트론 끼리 서로 영향을 주고 받을 수 없습니다. 



![neuron](/home/j/Downloads/neuron.png)





## 1.2 텐서 연산과 미니배치의 활용



딥러닝에서의 텐서는 다차원 숫자배열 이라고 이해해도 큰 문제가 없습니다.

미니배치는 여러 데이터를 한꺼번에 처리하기위하여 사용 됩니다.



1.1에서  이야기한 하나의 퍼셉트론에 관한 식을 반복문을 사용하는 것보다 백터를 사용하면 간단하고 빠르게 구할 수 있어집니다.그래서 이를 다시 적으면 다음과 같아집니다.
$$
y_1=\mathbf{x_1}\mathbf{w_1} +b_1
$$
이를 단층 퍼셉트론 신경망은 퍼셉트론이 n개 있을 때 하나의 입력벡터 **x** 를 통해 출력 y가 n개 나 나옵니다. 그와 동시에 가충치 행렬 과 편행 벡터가 사용 되 출력 벡터 **y**는 다음과 같이 표현할 수 있습니다. 
$$
\mathbf{y}=\mathbf{x}\mathbf{W}+\mathbf{b}
$$
여기서 더 나아가 입력 백터를 미니배치를 사용하게 되면 입력벡터 **x**와 출력벡터 **y** 는 벡터에서 행렬로 표현할 수 있게 됩니다. 
$$
\mathbf{Y}=\mathbf{X}\mathbf{W}+\mathbf{b}
$$


## 1.3 신경망의 세가지 기본 출력 유형과 회귀 분석

출력유형에는 회귀 분석, 이진 판단, 선택 분류로 구성 됩니다.



회귀 분석은 어떤 특징값 하나를 숫자로 추정하여 출력

이진판단은 '예','아니오' 가운데 택하여 출력

선택 분류는 여러 후보중 하나를 골라 출력



인공지능 알고리즘의 출력은 이 세 유형의 반복과 혼합으로 이루어 집니다.



## 1.4 전복 고리 수 추정 문제

전복은 현미경으로 고리수를 세 나이를 알 수 있습니다. 

다만 이는 길고 지루한 작업입니다.

이를 보안하기 위해 성별이나 크기 부게 등 육안을 식별 가능하거나 측정가능한 속성값들을 활용하여 나이를 추정해보는 인공신경망을 만들어 볼 예정입니다.



## 1.5 회귀 분석과 평균 제곱오차 손실 함수

추정값이 따로 주어지는 정답에 비교할 때 얼마나 정확한지를 숫자로 보여주는 정량적 지표가 필요합니다.

회구 분석에는 보통 평균제곱오차를 평가 지표로 사용합니다.

평균제곱오차는 출력 각 성분에 대해 추정값과 정답 사이의 차이인 오차를 제곱한 뒤 모두 합해 전체 성분 수로 나눈 값입니다. 
$$
평균제곱오차 = (추정값 - 정답)^2/전체 성분수
$$
제곱을 하므로 값은 항상 0 이상이며 정확해질수록 오차가 줄어 0에 가까워 집니다. 즉 완전 일치하면 0이 됩니다.

이러한 성질의 평가지표를 손실함수 혹은 비용함수라고 합니다.



## 1.6 경사하강법과 역전파

경사하강법은 함수의 기울기를 반복 계산 하면서 이 기울기에 따라 함수 값이 낮아지는 방향으로 이동하는 기본적인 딥러닝 학습 알고리즘이다.

경사하강법은 순전파와 역전파 과정을 번갈아 수행하는 과정을 반복하면서 신경망 파라미터들을 원하는 방향으로 바꾸어나간다.

순전파란 입력 데이터에 대해 신경망 구조를 따라가면서 현재의 파라미터값들을 이용해 손실 함수값을 계산하는 과정

역전파란 순전파의 계산 과정을 역순으로 거슬러 가면서 손실 함숫값에 직간접적으로 영향을 미친 모든 성분에 대한 손실 기울기를 계산하는 과정입니다.

미니배치 입력 데이터에 대해 순전파와 역전파 과정을 반복하며 신경망 파라미터를 원하는 방향으로 바꾸어 나가는 것이다. 순전파는 입력된 데이터에 대해 신경망 구조를 따라가며 현재의 파라미터값들을 이용하여 손실 함수값을 계산하는 것을 말하며, 역전파는 순전파의 계산과정을 거꾸로 진행하며 손실 함수값을 계산하는 것을 말한다. 성분 x에 대한 손실 기울기는 그 성분에 대한 손실 함수값의 변화율을 말한다. 파라미터 성분데 대해서 계산된 손실 기울기를 이용하여 값을 변경한다. 이 변경으로 학습이 일어나게 된다. 경사하강법의 경우 학습률이라는 하이퍼파라미터값을 곱한 값을 빼는 방법으로 파라미터값을 변경하게 된다.

![경사하강법](/home/j/Downloads/경사하강법.png)

 그래프 전체를 볼 수 없는 상황에서 $$f(x)$$ 값이 최소가 되는 $x$를 찾아야한다. 그래프의 경사는 미분값 $f'(x)$ 에 해당하므로 이에 비례하는 값을 $x$에서 빼주는 처리를 하면 이 보든 처기가 가능하다. 이를 그래프에서 학습의 관계식을 나타내면    $x_{i+1} = x_i-\alpha{\partial f(x)\over\partial {x}} $가 된다. 이때의 $\alpha$를 비례상수 학습률이라고 하며 임의의 양수값을 사용할 수 있지만 값이 클수록 목표 근처에서 정확하게 바닥을 찾는 능력이 무디어지고 값이 작으루록 바닥점 근처에 접근하는 시간이 더 오래 걸린다.

식에서 전미분 $\operatorname{d}\!f(x)\over\operatorname{d}\!x $ 대신 편미분 $\partial f(x)\over\partial{x}$ 을 사용하는데 이는 간접적인 영향은 무시한채 직접적인영향 만을 따지기 때문에 더 쉽게 계산 할 수 있다. 또한 전미분은 대분분 계산 자체가 불가능하거나 몹시 어렵지만 ,편미분은 상대적으로 쉽게 계산 가능하다. 거기에 편미분 과정에서 상수처럼 취급되는 다름 변수도 같은 편미분 방식으로 처리해서 이들을 잘 종합하면 전체적으로 전미분과 같은 효과를 낼 수 있다.



경사하강법에서는 우선 난수함수등을 이용해 파라미터값을 초기화 한다. 이어서 순전파와 역전파를 반복하면서 파라미터값들을 조절해가는데 이는 미니배치 데이터를 이용해 현재 위치의 기울기 벡터를 추정한 후 그 방향으로 파라미터들을 한 걸음 움직여 주는 것과 같은 과정이다.



## 1.7 편미분과 손실 기울기의 계산

편미분의 연쇄적 관계 덕분에 모든 성분에 대해 손실 기울기를 계산할 수 있다. 이는 미분의 기본 성질인 다음 수식을 의미한다. ${\partial L \over \partial x} ={\partial L \over \partial y}{\partial y \over \partial x} $ 의 수식을 의미 한다.

순전파에서는 (1) $x$가 구해지며 (2)  $y=f(x)$ 를 구한다 (3) $y$를 이용하여 손실함수 $L$이 계산 된다.

역전파는 위의 순전파의 반대 방향으로 진행 된다. (1)${\partial L\over\partial L}= 1.0$으로부터 $y$의 손실기울기 $\partial L \over \partial y$ 를 구해 이 값을 전달하게 된다. (2)${\partial L \over \partial x}= {\partial L \over \partial y}{\partial y \over \partial x}={\partial L \over \partial y}{\partial f(x) \over \partial x}$ 로부터 입력 $x$의 손실 기울기 ${\partial L \over \partial x}$를 전달받아 역전파 처리에 이용 한다.

편미분에서는 다른 입력 번수끼리 는 서로 무관한 것으로 간주하기 때문에 이 경우의 이경우에는 각 입력 성분에 대한 손실 기울기를  역전파의 (2)와 같은방법 즉, ${\partial L \over \partial x_i} = {\partial L \over \partial y}{\partial f(x_1,...,x_n)}\over \partial x_i$ 로 계산한다. 그리고 이들 값을 모아 ${\partial L \over \partial x}= ({\partial L \over \partial x_1},...{\partial L \over \partial x_n})$ 으로 전달하면 된다.

 $\frac{\sigma L}{\sigma x}={\frac{\sigma L}{\sigma x_1},...,\frac{\sigma L}{\sigma x_n}}$ 하나의 출력이 중복으로 이용되는 경우도 있다. 이경우에도 위 처럼 출력의 손실 기울기를 단순히 합산하여 손실 기울기 계산에 반영하면 된다.



## 1.8 하이퍼 파라미터

1. 입력 데이터

   학습에 사용되는 데이터처럼 외부에서 주어지는 값이며 개발자 입장에서는 직접 손댈 수 없는 고정된 값이다.

2. 파라미터

   순전파나 역전파 처리 과정에서 생성되는 각종 중간 계산 결과이다. 학습 과정에서 최소화 대상이 되는 손실 함수 값이나 성능에 대한 평가 지표로서 신경망의 출력 유형에 따라 별도로 정의 될 정확도는 지속적인 관찰의 대상이 되기도 한다. 하지만 이 값은 실행 결과로써 얻어 지므로 좋은 값이 나오도록 할 뿐 달리 손댈 방법이 없다.

3. 중간 계산 결과

   순전파과 역전파 처리에 계속 이용되는값이다. 퍼셉트론의 가중치나 편향이 이에 해당한다. 결국 파라미터의 구조를 잘 잡는 방법과 값을 적절한 조합으로 만드는 학습 방법을 탐구하는 것으로 요약할 수 있다. 딥러닝의 알고리즘에서 이것은 핵심 요소이다.

4. 하이퍼 파라미터

   학습률이나 학습 횟수, 미니배치 크기처럼 딥러닝 모델의 구조나 학습 과정에 영향을 미치는 각종 상숫값이다. 실행하는 동안 값이 변하지 않는 상수 이지만 그렇다고 항상 고정불변인 것은 아니다. 마음에 드는 학습결과를 얻기까지 개발자가 끊임없이 값을 바꾸어가며 실험한다. 이는 개발자가 미리 정해 줘야한느 값으로 많은 이해와 경험이 필요하다.





## 1.9 비선형 정보와 원-핫 벡터 표현

아발로니 데이터셋에 전복의 암수는 비선형 정보인 I, M, F로 표현한다. 신경망에서 처리하려면 이 정보를 숫자로 표현해야하는데 성별 정보는 숫자로 표현해도 선형정보로 볼 수 없다.

이를 표현하는 바람직한 방법은 유충, 수컷, 암컷 여부를 새개의 입력 항으로 분할하여 각각을 0과 1로 표혐하는것이다. 비선영 정보를 이렇게 항목별로 분할하여 해당 항목만1, 나머지는 0으로 나타내는 방식을 **원핫 벡터** 표현이라고 한다.



