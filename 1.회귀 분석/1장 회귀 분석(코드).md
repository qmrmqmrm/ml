```python
RND_MEAN = 0
RND_STD = 0.0030

LEARNING_RATE = 0.001
```

RND_MEAN 은정규분포의 난수값의 평균 RND_STD 표준편차이다. 가중치 파라미터를 초기화 할때 이용한다.

LEARNING_RATE는 하이퍼파라미터인 학습률이다.

```python
def abalone_exec(epoch_count=10, mb_size=10, report=1):
    load_abalone_dataset()
    init_model()
    train_and_test(epoch_count, mb_size, report)
```

load_abalone_dataset()은 데이터셋을 읽어드리는 함수

ini_model()은 모델의 파라미터들을 초기화하는 함수

train_and_test() 는 학습및 평가 과정을 수행하는 함수 이고

이들을 실행시키는 함수 abalone_exec는 학습횟수, 미니배치 크기, 중간보고 주기등 학습과정에 관련된 하이퍼파라미터값들을 매개변수로 지정받아 실제로 사용하는 train_and_test()함수에 전달한다.



```python
def load_abalone_dataset():
    with open('datasets_1495_2672_abalone.data.csv') as csvfile:
        csvreader = csv.reader(csvfile)
        next(csvreader, None)
        rows = []
        for row in csvreader:
            rows.append(row)
    #print(rows)
    global data, input_cnt, output_cnt
    input_cnt, output_cnt = 10, 1
    data = np.zeros([len(rows), input_cnt + output_cnt])

    for n, row in enumerate(rows):
        if row[0] == 'I': data[n, 0] = 1
        if row[0] == 'M': data[n, 1] = 1
        if row[0] == 'F': data[n, 2] = 1
        data[n, 3:] = row[1:]
```

이 함수는 csv 모듈의 기능을 이용해 아발로니 데이터셋을 가져와 rows 행렬로 만든다.

input_cnt, output_cnt의 값은 입력벡터와 출력 벡터의 크기를 의미하고

이를 이용하여 정보를 저장할 빈 data행렬을 만든다.

그후 비선형인 성별정보를 원핫벡터표현으로 변환한다.  row[0] 는 각 입력 데이터의 첫번째값인 성별 정보인데 이를 우리가 사용할 data에는 I는 0번쨰 M은 1번쨰 F는 2번째에 1로 변환하는 방식이다. 그후 다른 값들은 data의 3번째 값부터 이어 붙이게 된다.





```python
def init_model():
    global weight, bias, input_cnt, output_cnt
    weight = np.random.normal(RND_MEAN, RND_STD, [input_cnt, output_cnt])
    bias = np.zeros([output_cnt])
```

이 함 수에서는 단층 퍼셉틀노의 가중치 파라미터 weight와 편향 파라미터 bais를 초기화 한다. 이 예제에서는 weight는 (10,1) bias 는 [1] 형태로 만들어진다.

여기서 weight를 얻기 위해  정규분포 난숫값을 만들어주는 함수np.random.normal를 사용하였다.



```python
def train_and_test(epoch_count, mb_size, report):
    step_count = arrange_data(mb_size)
    test_x, test_y = get_test_data()
    # print(test_x,test_y)
    for epoch in range(epoch_count):
        losses, accs = [], []

        for n in range(step_count):
            train_x, train_y = get_train_data(mb_size, n)
            # print(train_x,train_y)
            loss, acc = run_train(train_x, train_y)
            # print(acc)
            losses.append(loss)
            accs.append(acc)
        # print(losses,"\n",accs,"\n",np.mean(losses), np.mean(accs), acc)
        if report > 0 and (epoch + 1) % report == 0:
            acc = run_test(test_x, test_y)
            # print(acc)
            print('Epoch {}: loss={:5.3f}, accuracy = {:5.3f}/{:5.3f}'.
                  format(epoch + 1, np.mean(losses), np.mean(accs), acc))

    final_acc = run_test(test_x, test_y)
    print('\nFinal Test: fianl accuracy = {:5.3f}'.format(final_acc))
```

여기서는 이중 반복을 이용해 에포크 만큼 학습을 반복하며 step_count값 만큼미니배치 처리를 반복한다.



미니배치 처리는 get_train_data()  함수로 학습용 미니배치 데이터를 얻어와 run_train()함수로 학습시키는 방식으로 처리되며 이때 미니배치 단위에서 비용과 정학도를 보고 받아 리스트 변수  loss와 accs를 집계한다.

 

각 에포크 처리가 끝나면 report 인수로 지정된 보고 주기에 해당하는지 검사한다.

해당되면 run_test()를 호출한 후 그 결과를 출력한다

또한 전체 에포크 처리가 끝나면 최종평가 결과를 출력한다.



```python
def arrange_data(mb_size):
    global data, shuffle_map, test_begin_idx
    shuffle_map = np.arange(data.shape[0])
    np.random.shuffle(shuffle_map)
    step_count = int(data.shape[0] * 0.8) // mb_size
    test_begin_idx = step_count * mb_size
    return step_count
```

data.shape[0]의 값만큼 일련번호를 발생시켜 무작위로 섞dj shuffle_map 으로 저장한다.

학습용 데이터와 평가용 데이터의 경계를 test_begin_idx에 저장 후 미니배치 처리 스텝수를 반환한다.

```python
def get_test_data():
    global data, shuffle_map, test_begin_idx, output_cnt
    test_data = data[shuffle_map[test_begin_idx:]]
    return test_data[:, :-output_cnt], test_data[:, -output_cnt:]
```

이 함수는 arrange_data에서 정한 test_begin_idx 를 경계삼아 shuffle_map 후반부가 가리키는 위치의 data 행들을 평가용 데이터로서 반환한다. 이때 뒤에서 output_cnt번째 되는 원소위치를 기준으로 분할해 앞쪽을 입력 벡터 뒷쪽을 정답 벡터로 반환한다.

```python
def get_train_data(mb_size, nth):
    global data, shuffle_map, test_begin_idx, output_cnt
    if nth == 0:
        np.random.shuffle(shuffle_map[:test_begin_idx])
    train_data = data[shuffle_map[mb_size * nth:mb_size * (nth + 1)]]
    return train_data[:, :-output_cnt], train_data[:, -output_cnt:]
```

미니배치 구간의 위치를 따져 그 구간에 해당하는 shuffle_map이 가리키는 데이터들만을 반환한다. 또한 nth 가 0 일때 에 한하여 test_begin_idx를 뒤섞어 에포크마다 다른 순서로 학습이 수행되게 한다. 반확값은 각 행에 대해 입력 베터 부분과 정답 벡터 부분을 분할해 반환한다.



```python
def run_train(x, y):
    output, aux_nn = forward_neuralnet(x)
    loss, aux_pp = forward_postproc(output, y)
    accuracy = eval_accuracy(output, y)

    G_loss = 1.0
    G_output = backprop_postproc(G_loss, aux_pp)
    backprop_neuralnet(G_output, aux_nn)

    return loss, accuracy
```

여기서는 학습용 데이터릐 일부로 주어지는 미니 배치 입력 행렬 x와 정답행렬 y를 이용해 한스텝의 학승ㄹ 수행한다. 처음 두줄은 순전파 처리고 세번째 줄은 정확도 계산 그뒤 네번째 다섯번째는 역전파 처리 순으로 진행된다.

먼저 forward_neuralnet() 함수가 단층 퍼셉트론 신경망에 대한 순전파를 수행하여 입력 행렬 x 로부터 신경망 출력  output을 구한다. 이어서 forward_postproc() 함수가 회구 분석 문제의 성격에 맞춘 후처리 순전파 적업을 수행하여 output과 y로 부터 손실 함수 loss를 계산한다.

이후 eval_accuracy함수를 호출해 정확도를 구한 후 역전파 처리 후에 반환 할 수 있도록 저장한다.

G_loss는 ${\partial L\over\partial L}= 1.0$ 이고 이부분이 역전파의 시작이다.

```python
def forward_neuralnet(x):
    global weight, bias
    output = np.matmul(x, weight) + bias
    return output, x


def backprop_neuralnet(G_output, x):
    global weight, bias
    g_output_w = x.transpose()

    G_w = np.matmul(g_output_w, G_output)
    G_b = np.sum(G_output, axis=0)

    weight -= LEARNING_RATE * G_w
    bias -= LEARNING_RATE * G_b
```

forward_neuralnet는 입력 행렬 x에 대한 가중치 행렬 weight를 곱하고 편향벡터 bias를 더하는 간단한 방법으로 신경망 출력에 해당하는 output 행렬을 만든다. 이때 가중치 곱샘은 행렬끼리 의 곱셈이고 편향 덧셈은 행렬과 벡터의 뎃셈이다.

backprop_neuralnet 함수는 역전파 처리를 수행한다.

