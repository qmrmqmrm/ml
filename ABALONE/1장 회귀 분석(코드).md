```python
        self.RND_MEAN = 0
        self.RND_STD = 0.0030
        self.LEARNING_RATE = 0.1
```

RND_MEAN 은정규분포의 난수값의 평균 RND_STD 표준편차이다. 가중치 파라미터를 초기화 할때 이용한다.

LEARNING_RATE는 하이퍼파라미터인 학습률이다.

```python
    def abalone_exec(self, epoch_count=10, mb_size=10, report=20):
        self.load_abalone_dataset()
        self.init_model()
        self.train_and_test(epoch_count, mb_size, report)
```

load_abalone_dataset()은 데이터셋을 읽어드리는 함수

ini_model()은 모델의 파라미터들을 초기화하는 함수

train_and_test() 는 학습및 평가 과정을 수행하는 함수 이고

이들을 실행시키는 함수 abalone_exec는 학습횟수, 미니배치 크기, 중간보고 주기등 학습과정에 관련된 하이퍼파라미터값들을 매개변수로 지정받아 실제로 사용하는 train_and_test()함수에 전달한다.



```python
    def load_abalone_dataset(self):
        with open('./abalone.csv') as csvfile:
            csvreader = csv.reader(csvfile)
            next(csvreader, None)
            rows = []

            for row in csvreader:
                rows.append(row)
            self.input_cnt, self.output_cnt = 10, 1
            self.data = np.zeros([len(rows), self.input_cnt + self.output_cnt])
            for n, row in enumerate(rows):
                if row[0] == 'I': self.data[n, 0] = 1
                if row[0] == 'M': self.data[n, 1] = 1
                if row[0] == 'F': self.data[n, 2] = 1
                self.data[n, 3:] = row[1:]
```

이 함수는 csv 모듈의 기능을 이용해 불러온 아발로니 데이터셋을 next함수로 첫행을 읽지않고 가져와 rows 리스트로 만든다. 이때 rows는 (4177,9)의 형태를 같는다. 이는 쉽게 측정가능한 8가지 특징값(성별,키,지름,높이,전체무게,몸통무게,내장무게,껍질무게)과 측정한 껍질 고리수가 들어있다. 

input_cnt, output_cnt의 값은 입력벡터와 출력 벡터의 크기를 의미하고

이를 이용하여 정보를 저장할 np.zero형태의 data행렬(4177, 11)을 만든다.

그후 비선형인 성별정보를 원핫벡터표현으로 변환한다.  row[0] 는 각 입력 데이터의 첫번째값인 성별 정보인데 이를 우리가 사용할 data에는 Infant는 0번쨰 Male은 1번쨰 Female는 2번째에 1로 변환하는 방식이다. 그후 다른 값들은 data의 3번째 값부터 이어 붙이게 된다.

먼저 엑셀에서 데이터셋을 가져옵니다. 이때 데이터는 (4177,9) 이중 성별을 3가지 원핫 벡터로 변환하여 (4177,11)의 data 행렬로 변환하게 됩니다. 

이떼 11개의 데이터중 11 번째 데이터는 정답값으로 사용하게 되고 나머지 데이터는 입력 데이터로 사용 됩니다.





```python
def init_model(self):
    self.weight = np.random.normal(self.RND_MEAN, self.RND_STD, 
                                   [self.input_cnt,self.output_cnt])
    self.bias = np.zeros([self.output_cnt])
```

이 함 수에서는 단층 퍼셉틀노의 가중치 파라미터 weight와 편향 파라미터 bais를 초기화 한다. 이 예제에서는 weight는 (10,1) bias 는 (1,) 형태로 만들어진다.

여기서 weight를 얻기 위해  정규분포 난숫값을 만들어주는 함수np.random.normal를 사용하였다.

경사하강법의 출발점에 해당하는 파라미터의 초깃값을 실행할 때마자 달라지게 만들기 휘한 의도이다.

편향은 초기에 지나친 영향을 주어 학습에 역효과를 불러오지 않도록 0으로 초기화한다.

weight는 (10,1) bias 는 0으로 초기화 합니다.





```python
    def train_and_test(self, epoch_count, mb_size, report):
        step_count = self.arrange_data(mb_size)
        test_x, test_y = self.get_test_data()

        for epoch in range(epoch_count):
            losses, accs = [], []

            for n in range(step_count):
                train_x, train_y = self.get_train_data(mb_size, n)
                loss, acc = self.run_train(train_x, train_y)
                losses.append(loss)
                accs.append(acc)

            if report > 0 and (epoch + 1) % report == 0:
                acc = self.run_test(test_x, test_y)
                print('Epoch {}: loss={:5.3f}, accuracy = {:5.3f}/{:5.3f}'.
                      format(epoch + 1, np.mean(losses), np.mean(accs), acc))

        final_acc = self.run_test(test_x, test_y)
        print('\nFinal Test: fianl accuracy = {:5.3f}'.format(final_acc))

```

여기서는 이중 반복을 이용해 epoch 만큼 학습을 반복하며 step_count값 만큼 미니배치 처리를 반복한다.

미니배치 처리는 get_train_data()  함수로 학습용 미니배치 데이터를 얻어와 run_train()함수로 학습시키는 방식으로 처리되며 이때 미니배치 단위에서 비용과 정확도를 보고 받아 리스트 변수  loss와 accs를 집계한다.

각 에포크 처리가 끝나면 report 인수로 지정된 보고 주기에 해당하는지 검사한다.

해당되면 run_test()를 호출한 후 그 결과를 출력한다

또한 전체 에포크 처리가 끝나면 최종평가 결과를 출력한다.



```python
    def arrange_data(self, mb_size):
        self.shuffle_map = np.arange(self.data.shape[0])
        np.random.shuffle(self.shuffle_map)
        step_count = int(self.data.shape[0] * 0.8) // mb_size
        self.test_begin_idx = step_count * mb_size
        return step_count
```

data.shape[0]의 값만큼 일련번호를 발생시켜 무작위로 섞어 shuffle_map [ 301 3805  291 ...  969 3408 3397]\(4177개)으로 저장한다.

step_count(334) 는 전체 데이터수에 80%를 기준으로 미니배치 싸이즈 만큼 나누어준다.

(step_count 는 총 미니배치 갯수입니다.)

test_begin_idx(3340)는 학습용 데이터와 평가용 데이터의 경계 나누는 기준으로 사용한다.

미니배치 처리 스텝수step_count를 반환한다.

```python
    def get_test_data(self):
        test_data = self.data[self.shuffle_map[self.test_begin_idx:]]
        return test_data[:, :-self.output_cnt], test_data[:, -self.output_cnt:]
```

이 함수는 arrange_data에서 정한 test_begin_idx 를 경계삼아 shuffle_map 후반부가 가리키는 위치의 data 행들을 평가용 데이터test_data(837, 11)로서 반환한다. 이때 각test_data의 행에서 뒤에서 output_cnt번째(1번째) 되는 원소위치를 기준으로 분할해 앞쪽을 입력 벡터 뒷쪽을 정답 벡터로 반환한다.

```python
    def get_train_data(self, mb_size, nth):
        if nth == 0:
            np.random.shuffle(self.shuffle_map[:self.test_begin_idx])
        train_data = self.data[self.shuffle_map[mb_size * nth:mb_size * (nth + 1)]]
        return train_data[:, :-self.output_cnt], train_data[:, -self.output_cnt:]
```

미니배치 구간의 위치를 따져 그 구간에 해당하는 shuffle_map이 가리키는 데이터들만을 반환한다. 또한 nth 가 0 일때 즉 각 epoch의 처음에 한하여 test_begin_idx를 뒤섞어 epoch마다 다른 순서로 학습이 수행되게 한다. 반환값은 각 행에 대해 입력 행렬 **X**부분(10, 10)과 정답 헁렬**Y** 부분(10, 1)을 분할해 반환한다.



```python
    def run_train(self, x, y):
        output, aux_nn = self.forward_neuralnet(x)
        loss, aux_pp = self.forward_postproc(output, y)//3
        accuracy = self.eval_accuracy(output, y)

        G_loss = 1.0
        G_output = self.backprop_postproc(G_loss, aux_pp)
        self.backprop_neuralnet(G_output, aux_nn)

        return loss, accuracy
    
    def run_test(self, x, y):
        output, _ = self.forward_neuralnet(x)
        accuracy = self.eval_accuracy(output, y)
        return accuracy
```

여기서는 학습용 데이터의 일부로 주어지는 미니 배치 입력 행렬 **X**와 정답행렬 **Y**를 이용해 한스텝의 학습을 수행한다. 처음 두줄은 순전파 처리고 세번째 줄은 정확도 계산 그뒤 네번째에서 여섯번째는 역전파 처리 순으로 진행된다.

먼저 forward_neuralnet() 함수가 단층 퍼셉트론 신경망에 대한 순전파를 수행하여 입력 행렬 **X**(10,10) 로부터 신경망을 거친 신경망 출력  **output**(10,1)을 구한다. 이어서 forward_postproc() 함수가 회귀 분석 문제의 성격에 맞춘 후처리 순전파 적업을 수행하여 **output**과 **y**로 부터 손실 함수 loss를 계산한다.

이후 eval_accuracy함수를 호출해 정확도를 구한 후 역전파 처리 후에 반환 할 수 있도록 저장한다.

이과정에서 추가로 반환된 aux_nn과 aux_pp는 역전파 과정에서 사용될 순전파 계산과정에서 확보 할 수 있는 값들이다.

G_loss는 ${\partial L\over\partial L}= 1.0$ 이고 이부분이 역전파의 시작이다. 이후  backprop_postproc() $y$의 손실기울기 $\partial L \over \partial y$ G_output을 구합니다. 그후 backprop_neuralnet 함수로 전달하여 신경망 파라미터값의 변화 즉 학습이 실제로 일어난다.

run_test함수는 순전파만 수행하후 바로 eval_accuracy함 수를 호출해 정확도를 계산해 반환한다.

```python
    def forward_neuralnet(self, x):
        output = np.matmul(x, self.weight) + self.bias
        return output, x

    def backprop_neuralnet(self, G_output, x):
        g_output_w = x.transpose()

        G_w = np.matmul(g_output_w, G_output) 
        G_b = np.sum(G_output, axis=0)

        self.weight -= self.LEARNING_RATE * G_w
        self.bias -= self.LEARNING_RATE * G_b
```

forward_neuralnet는 입력 행렬 x에 대한 가중치 행렬 weight를 곱하고 편향벡터 bias를 더하는 간단한 방법으로 신경망 출력에 해당하는 output 행렬을 만든다. 이때 가중치 곱샘은 행렬끼리 의 곱셈이고 편향 덧셈은 행렬과 벡터의 뎃셈이다.

backprop_neuralnet 함수는 역전파 처리를 수행한다. 순전파 출력  output에 관한 손실 기울기 G_output 을 전달 받아 weight 와 bias 성분의 손실 기울기 G_w G_b를 구한다. 여기에 학습률을 곱한값을 weight 와 bias에서 빼 줌으로써 가중치와 편향 파라미터에 대한 실제 학습을수행한다.

이 과정에서 가중치 손실 기울기는  G_output과 **W**가 loss에예 미치는 영향가운데 y를 통해 미치는 부분만 계산하

수식이 어떻게 구현 

np.matmul은 행렬 곱을 하기 위해 사용된다.

```python
    def forward_postproc(self, output, y):
        diff = output - y
        square = np.square(diff)
        loss = np.mean(square)
        return loss, diff

    def backprop_postproc(self, G_loss, diff):
        shape = diff.shape

        g_loss_square = np.ones(shape) / np.prod(shape)
        g_square_diff = 2 * diff
        g_diff_output = 1

        G_square = g_loss_square * G_loss
        G_diff = g_square_diff * G_square
        G_output = g_diff_output * G_diff

        return G_output
```

forwart_postproc함수에서는 평균제곱오차를 구하는 연산을 수행한다. 출력 output과 정답 행렬 y에 대해 각 행렬 원소 짝에 대한 오차와 그 제곱을 diff와 square에 차례로 구하고, 이들의 평균으로  loss를 구한다. 



backprop_postproc()는 1.0으로 설정된 채 매개 변수로 전달 되는 G_loss 값을 이용해 순전파 역순으로 G_output을 구해 반환한다.

L =()

```python
    def backprop_postproc_oneline(self, G_loss, diff):
        return 2 * diff / np.prod(diff.shape)
```

backprop_postproc함수를 한줄로 요약한 함수입니다.

```python
    def eval_accuracy(self, output, y):
        mdiff = np.mean(np.abs((output - y) / y))
        return 1 - mdiff
```

와 ${\partial L\over\partial {b}}$=G_{output}의 합 를 구한 후 $w_{i+1} = w_i-\alpha{\partial L\over\partial {w}} $ 와 $b_{i+1} = x_i-\alpha{\partial L\over\partial {b}} $ 를 구하게 되는데 ${\partial L\over\partial {w}}=X^T{\partial L\over\partial {output}}$  를 이용하여 구하게 됩니다. 

