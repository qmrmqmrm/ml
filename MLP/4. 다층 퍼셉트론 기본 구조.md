# 4. 다층 퍼셉트론 기본 구조

이전에 사용한 코드는 단층 퍼셉트론을 기준으로 만들어 졌습니다. 또한 각 장에서는 회귀 분석, 이진 판단, 선택 분류 문제를 풀어 내기 위한 후처리 과정을 변경하였을 뿐입니다. 그리고 전처리와 init 함수에서는 단층 퍼셉트론에 맞게 구성 되어있었습니다.

이를 다층 퍼셉트론으로 변경하는 작업은 신경망 내부의 처리 과정에 직접적으로 관련된 전처리와 init함수를 재 정의 하는 것으로 충분합니다.

```python
def init_model_hidden1(self):
    # print("mlp:init_model_hidden1")
    self.pm_hidden = self.alloc_param_pair([self.input_cnt, self.hidden_cnt])
    self.pm_output = self.alloc_param_pair([self.hidden_cnt, self.output_cnt])

def alloc_param_pair(self, shape):
    # print("mlp:alloc_param_pair")
    weight = np.random.normal(self.RND_MEAN, self.RND_STD, shape)
    bias = np.zeros(shape[-1])
    return {'w': weight, 'b': bias}
```

먼저 은닉 계층이 하나인 다층 퍼셉트론 신경망의 모델 파라미터를 생성하는 함수입니다.

싱경망의 각 계층에서는 각각 가중치 행렬과 편향 벡터로 구성되는 파라미터 쌍이 필요로 합니다. 그렇기 때문에 출력 계층과 은닉 계층을 하나씩 가지는 두 쌍의 파라미터가 필요로 합니다. 이때 파라미터를 구성하는 내용은 중복되므로 따로 함수로 만들어 주어 두번 호출 되고 있습니다. 또한 파라미터를 dictionary 형태로 반환 해주고 있습니다.

또한 파라미터 크기를 지정하기 위해 은닉 계층은 input_cnt와 hidden_cnt를 출력 계층은 hidden_cnt  와 output_cnt를 각각 입출력 벡터 크기로 이용합니다. 이를 리스트로 묶어 파라미터 생성 함수로 전달 하는데 이때 2차원 형태를 넘어서는 경우에 대비하여 편향 벡터의 크기를 shape[-1]의 형태로 이용됩니다.

예로 전복데이터를 사용할때 set_hidden(4)를 할 경우 은닉계층의 가중치는 (10,4)  bias는 (4,) 출력계층의 가중치는 (4,1) bias는 (1,) 형태를 가지게 됩니다.

```python
def forward_neuralnet_hidden1(self, x):
    # print("mlp:forward_neuralnet_hidden1")
    hidden = self.relu(np.matmul(x, self.pm_hidden['w']) + self.pm_hidden['b'])
    output = np.matmul(hidden, self.pm_output['w']) + self.pm_output['b']

    return output, [x, hidden]

def relu(self, x):
    # print("mlp:relu")
    return np.maximum(x, 0)
```

이 함수에서는  위에서 생성한 은닉계층과 출력 계층 파라미터을 이용하여 은닉계층을 구할때입력 x와 pm_hidden을 이용하여 출력hidden을 계산하고 이어서 출력 계층에서 hidden과 pm_output을 이용하여 출력이자 신경망의 최종 출력인 output을 계산하게 됩니다. 

이때 은닉계층을 구하기 위하여 Relu 함수를 사용하게 됩니다. 또한 출력 계층의 역전파처리 때 가중치에 대한 편미분 정보로 x 가 필요한것처럼 은닉계층의 역전파 처리할 때에 hidden이 필요하기 때문에 x, hidden 또한 반환 하고 있습니다.

```python
def backprop_neuralnet_hidden1(self, G_output, aux):
    # print("mlp:backprop_neuralnet_hidden1")
    x, hidden = aux

    g_output_w_out = hidden.transpose()
    G_w_out = np.matmul(g_output_w_out, G_output)
    G_b_out = np.sum(G_output, axis=0)

    g_output_hidden = self.pm_output['w'].transpose()
    G_hidden = np.matmul(G_output, g_output_hidden)

    self.pm_output['w'] -= self.LEARNING_RATE * G_w_out
    self.pm_output['b'] -= self.LEARNING_RATE * G_b_out

    G_hidden = G_hidden * self.relu_derv(hidden)

    g_hidden_w_hid = x.transpose()
    G_w_hid = np.matmul(g_hidden_w_hid, G_hidden)
    G_b_hid = np.sum(G_hidden, axis=0)

    self.pm_hidden['w'] -= self.LEARNING_RATE * G_w_hid
    self.pm_hidden['b'] -= self.LEARNING_RATE * G_b_hid
    
    
def relu_derv(self, y):
    # print("mlp:relu_derv")
    return np.sign(y)
```

두 개의 계층을 처리 하지만 기존의 과정을 은닉계층과 출력계층으로 반복하고 있습니다. 먼저
$$
{\partial L \over \partial w_o} = {\partial L\over \partial output }{\partial output \over \partial w_o}=G_{output}H=H^{T}G_{output}\\
{\partial L \over \partial b_o} = \sum{G_{output}}
$$
로 출력 계층의 가중치와 편향값을 업데이트합니다.

하지만 은닉계층을 업데이트에 순전파 때 이용 되었던 출력계층 가중치가 필요 하므로 출력 계층 가중치 업데이트 전에 추가적이 계산이 들어갑니다. 이때 은닉 계층을 계산 할때 필요한 
$$
{\partial L \over \partial H} = {\partial L \over \partial output}{\partial output \over \partial H} = G_{output}W_o = W_o^T G_{output}= G_{hidden}\\
{\partial L \over \partial output} = G_{output}\\
{\partial output \over \partial H} = W_o
$$
를 구하게 되고 그후에 ReLU의 역전파 처리를 하게 되고 이어서 은닉계층의 가중치 업데이트가 되게 됩니다.
$$
{\partial L \over \partial w_h} ={\partial L \over \partial H}{\partial H \over \partial w_h} = G_{hidden}X = X^TG_{hidden} \\ 
{\partial L \over \partial b_h} =\sum{G_{hidden}}
$$


여기까지가 하나의 은닉 계층만을 지원하는 구현이였습니다.

이와달리 은닉계층의 수와 폭을 임의로 바꿀 수 있는 가변적 다층 퍼셉트론 신경망 구조를 지원 할 함수들을 정의 했습니다.

```python
def init_model_hiddens(self):
    # print("mlp:init_model_hiddens")
    self.pm_hiddens = []
    prev_cnt = self.input_cnt

    for hidden_cnt in self.hidden_config:
        self.pm_hiddens.append(self.alloc_param_pair([prev_cnt, hidden_cnt]))
        prev_cnt = hidden_cnt

    self.pm_output = self.alloc_param_pair([prev_cnt, self.output_cnt])
```

여기서는 출력계층은 위와 같이 하고 은닉계층을 생성하는 부분에서 하나가 아닌 리스트 형식으로 은닉계층의 파라미터 값들을 저장하게 됩니다.  이때 prev_cnt 변수를 이용하여 은닉 계층의 폭, 즉 출력 벡터 크기로 이용 된 hidden_cnt 값을 저장 하여 다음 계층에 전달 한다. 마지막 은닉 계층의 출력 벡터 크기를 갖게 되며 출력 계층의 입력 벡터 크기로 이용됩니다.

```python
def forward_neuralnet_hiddens(self, x):
    # print("mlp:forward_neuralnet_hiddens")
    hidden = x
    hiddens = [x]

    for pm_hidden in self.pm_hiddens:
        hidden = self.relu(np.matmul(hidden, pm_hidden['w']) + pm_hidden['b'])
        hiddens.append(hidden)

    output = np.matmul(hidden, self.pm_output['w']) + self.pm_output['b']

    return output, hiddens
```

여기서도 마찬가지로 은닉계층이 하나 일때 와 동일 하지만 은닉계층의 수만큼 반복 해주게 됩니다. 이과정에서 hidden 변수에 계산될 은닉계층을 대입하여 다음 계층을 계산 할 때 다시 이용되게 하여 이전 계층과 다음 계층과의 매개하는 역할입니다. 또한 이 은닉계층을 하나의 리스트로 만들어 역전파를 할때 사용할 보조 정보로 전달하게 됩니다.

```python
def backprop_neuralnet_hiddens(self, G_output, aux):
    # print("mlp:backprop_neuralnet_hiddens")
    hiddens = aux

    g_output_w_out = hiddens[-1].transpose()
    G_w_out = np.matmul(g_output_w_out, G_output)
    G_b_out = np.sum(G_output, axis=0)

    g_output_hidden = self.pm_output['w'].transpose()
    G_hidden = np.matmul(G_output, g_output_hidden)

    self.pm_output['w'] -= self.LEARNING_RATE * G_w_out
    self.pm_output['b'] -= self.LEARNING_RATE * G_b_out

    for n in reversed(range(len(self.pm_hiddens))):
        G_hidden = G_hidden * self.relu_derv(hiddens[n + 1])

        g_hidden_w_hid = hiddens[n].transpose()
        G_w_hid = np.matmul(g_hidden_w_hid, G_hidden)
        G_b_hid = np.sum(G_hidden, axis=0)

        g_hidden_hidden = self.pm_hiddens[n]['w'].transpose()
        G_hidden = np.matmul(G_hidden, g_hidden_hidden)

        self.pm_hiddens[n]['w'] -= self.LEARNING_RATE * G_w_hid
        self.pm_hiddens[n]['b'] -= self.LEARNING_RATE * G_b_hid
```

역전파에서도 출력계층에 관한 가중치 업데이트는 하나일때와 같이 하고 은닉층은 역순으로 진행을 해야 합니다. 이때  출력 계층 가중치를 업데이트 할때 사용할 은닉계층의 마지막 층을 hiddens[-1]을 사용하여 은닉층의 마지막을 불러와 사용했습니다.  또한 reversed 함수를 사용하여 반복문 실행 순서를 반대로 하여 은닉 계층의 역전파를 수행하였습니다.

```python
def init_model(self):
    print("mlp:init_model")
    # print(self.hidden_config)
    if self.hidden_config is not None:
        print('은닉 계층 {}개를 갖는 다층 퍼셉트론이 작동되었습니다.'. \
              format(len(self.hidden_config)))
        self.init_model_hiddens()
    else:
        print('은닉 계층 하나를 갖는 다층 퍼셉트론이 작동되었습니다.')
        self.init_model_hidden1()

def forward_neuralnet(self, x):
    # print("mlp:forward_neuralnet")
    # print(x.shape)
    if self.hidden_config is not None:
        return self.forward_neuralnet_hiddens(x)
    else:
        return self.forward_neuralnet_hidden1(x)

def backprop_neuralnet(self, G_output, hiddens):
    # print("mlp:backprop_neuralnet")
    if self.hidden_config is not None:
        self.backprop_neuralnet_hiddens(G_output, hiddens)
    else:
        self.backprop_neuralnet_hidden1(G_output, hiddens)
```

여기서는 hidden_config 변수의 설정여부에 따라 하나의 은닉 계층을 사용할지 가변적 은닉계층에 관한 함수들을 호출할지 결정이 된다.

```python
def set_hidden(self, info):
    # print("mlp:set_hidden")
    if isinstance(info, int):
        # print(f"if:{info}")
        self.hidden_cnt = info
        self.hidden_config = None
    else:
        self.hidden_config = info
```

여기서 주워진 info에 따라 int이면 hidden_config가 None으로 설정되어 하나의 은닉계층을 사용하도록 int가 아닌 값이 들어오면 가변적 은닉계층에 관한 함수들로 호출할 지 결정이 된다.  예로 1장의 데이터를 사용하게 되면

```python
def main():
    mlp = Mlp()
    mlp.set_hidden([])
    mlp.abalone_exec()
```

이와 같이 사용하게 되면 hidden_config에 공백의 리스트가 들어가 은닉층이 생성이 안되 단층 퍼셉트론 처럼 동착을 하게 됩니다. 또한 

```python
def main():
    mlp = Mlp()
    mlp.set_hidden(4)
    mlp.abalone_exec(epoch_count=50 ,mb_size=20,report=10)
```

이처럼 int형식으로 넣게 되면 하나의 은닉층이 생성이되고 

은닉층의 가중치의 형태는 (10,4) 출력층의 형태는 (4,1)를 가지게 됩니다.

```
은닉 계층 하나를 갖는 다층 퍼셉트론이 작동되었습니다.
은닉층 weight shape (10, 4)
출력층 weight shape (4, 1)
Epoch 10: loss=6.746, accuracy = 0.808/0.803
Epoch 20: loss=6.230, accuracy = 0.817/0.811
Epoch 30: loss=5.579, accuracy = 0.827/0.829
Epoch 40: loss=5.136, accuracy = 0.836/0.839
Epoch 50: loss=4.978, accuracy = 0.839/0.832

Final Test: fianl accuracy = 0.832

Process finished with exit code 0
```

```python
def main():
    mlp = Mlp()
    mlp.set_hidden([6])
    mlp.pulsar_exec(epoch_count=50,report=10)
```

```
은닉 계층 1개를 갖는 다층 퍼셉트론이 작동되었습니다.
은닉층1 weight shape(8, 6)
출력층 weight shape (6, 1)
step1431
Epoch 10: loss=0.094, accuracy = 0.972/0.972
Epoch 20: loss=0.091, accuracy = 0.973/0.974
Epoch 30: loss=0.091, accuracy = 0.973/0.974
Epoch 40: loss=0.089, accuracy = 0.974/0.974
Epoch 50: loss=0.089, accuracy = 0.974/0.975

Final Test: fianl accuracy = 0.975

Process finished with exit code 0

```

```python
def main():
    mlp = Mlp()
    mlp.set_hidden([12,6])
    mlp.pulsar_exec(epoch_count=50,report=10)	
    
```

이와 같이 [12,6] 형태의 리스트를 넣게 되면 12게의 퍼셉트론으로 구성된 은닉 계층 하나 6개의 퍼셉트론으로 구성된 은닉 계층 하나가 생성됩니다.

```
은닉 계층 2개를 갖는 다층 퍼셉트론이 작동되었습니다.
은닉층1 weight shape(8, 12)
은닉층2 weight shape(12, 6)
출력층 weight shape (6, 1)
Epoch 10: loss=0.150, accuracy = 0.908/0.910
Epoch 20: loss=0.128, accuracy = 0.970/0.968
Epoch 30: loss=0.116, accuracy = 0.972/0.972
Epoch 40: loss=0.109, accuracy = 0.974/0.973
Epoch 50: loss=0.104, accuracy = 0.974/0.974

Final Test: fianl accuracy = 0.974

Process finished with exit code 0

```



